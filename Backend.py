from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import joblib
import numpy as np
import pandas as pd
import os
import io
from typing import Dict, Any, List, Optional, Union
from collections import Counter
import logging
from sklearn.base import BaseEstimator, TransformerMixin
from fastapi.responses import StreamingResponse

# ==================== MANUAL VECTORIZER ====================
# B·ªô bi·∫øn ƒë·ªïi (vectorizer) th·ªß c√¥ng, d√πng ƒë·ªÉ chuy·ªÉn vƒÉn b·∫£n th√†nh vector s·ªë
class ManualVectorizer(BaseEstimator, TransformerMixin):
    def __init__(self, vocab=None):
        # vocab: danh s√°ch c√°c t·ª´ ƒë∆∞·ª£c d√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh
        self.vocab = vocab or {}
        # T·∫°o ch·ªâ m·ª•c (index) cho t·ª´ng t·ª´ trong vocab
        self.vocab_index = {word: idx for idx, word in enumerate(self.vocab)}
    
    def fit(self, X, y=None):
        # H√†m fit kh√¥ng l√†m g√¨ v√¨ vocab ƒë√£ c√≥ s·∫µn
        return self
    
    def transform(self, X):
        # N·∫øu ch·ªâ truy·ªÅn v√†o 1 chu·ªói, ƒë∆∞a v√†o danh s√°ch
        if isinstance(X, str):
            X = [X]
        
        results = []
        for text in X:
            # T·∫°o vector ƒë·ªô d√†i = s·ªë t·ª´ trong vocab, kh·ªüi t·∫°o b·∫±ng 0
            features = np.zeros(len(self.vocab))
            words = text.lower().split()
            for word in words:
                # N·∫øu t·ª´ n·∫±m trong vocab th√¨ tƒÉng t·∫ßn su·∫•t
                if word in self.vocab_index:
                    features[self.vocab_index[word]] += 1
            results.append(features)
        
        return np.array(results)
    
    def fit_transform(self, X, y=None):
        return self.transform(X)

    @property
    def vocabulary_(self):
        # Tr·∫£ v·ªÅ t·ª´ ƒëi·ªÉn vocab_index (ph√π h·ª£p v·ªõi chu·∫©n sklearn)
        return self.vocab_index


# ==================== C·∫§U H√åNH LOGGING ====================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh v√† vectorizer
ARTIFACT_DIR = "artifacts"
VEC_PATH = os.path.join(ARTIFACT_DIR, "vectorizer.pkl")
MODEL_PATH = os.path.join(ARTIFACT_DIR, "spam_model.pkl")

# ==================== KH·ªûI T·∫†O FASTAPI ====================
app = FastAPI(
    title="AmongSMS - Spam Detection API",
    description="API ph√°t hi·ªán tin nh·∫Øn r√°c (spam)",
    version="1.0.0"
)

# ==================== C·∫§U H√åNH CORS ====================
# Cho ph√©p giao di·ªán web (frontend) ·ªü b·∫•t k·ª≥ ngu·ªìn n√†o c√≥ th·ªÉ g·ªçi API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p m·ªçi domain
    allow_credentials=True,
    allow_methods=["*"],  # Cho ph√©p t·∫•t c·∫£ c√°c lo·∫°i HTTP method
    allow_headers=["*"],  # Cho ph√©p t·∫•t c·∫£ headers
)

# ==================== ƒê·ªäNH NGHƒ®A KI·ªÇU D·ªÆ LI·ªÜU ====================
class SMSRequest(BaseModel):
    text: str  # N·ªôi dung tin nh·∫Øn ƒë·∫ßu v√†o

class SMSResponse(BaseModel):
    label: str              # K·∫øt qu·∫£ d·ª± ƒëo√°n: "spam" ho·∫∑c "ham"
    prob: Optional[float]   # X√°c su·∫•t l√† spam
    top_words: List[List[Union[str, int]]]  # T·ª´ kh√≥a xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
    confidence: float       # M·ª©c ƒë·ªô tin c·∫≠y (%)

class BatchResponse(BaseModel):
    filename: str
    total_messages: int
    spam_count: int
    ham_count: int
    results: List[Dict]

# ==================== CORE FUNCTIONS ====================
def load_artifacts():
    if not os.path.exists(VEC_PATH) or not os.path.exists(MODEL_PATH):
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y vectorizer.pkl ho·∫∑c spam_model.pkl trong artifacts")
    vec = joblib.load(VEC_PATH)
    model = joblib.load(MODEL_PATH)
    return vec, model

def _tokenize(text: str):
    text = text.lower()
    clean = "".join(ch if "a" <= ch <= "z" or ch == " " else " " for ch in text)
    return [w for w in clean.split() if w]

def extract_top_spam_words(text: str, vec=None, top_k: int = 10):
    words = _tokenize(text)
    if vec is None:
        vec, _ = load_artifacts()
    vocab = set(vec.vocab) if hasattr(vec, "vocab") else set(vec.vocab_index.keys())
    counter = Counter([w for w in words if w in vocab])
    top_items = counter.most_common(top_k)
    return top_items

# ==================== ENDPOINTS (C√ÅC ƒê∆Ø·ªúNG G·ªåI API) ====================

@app.get("/")
async def root():
    # Endpoint ki·ªÉm tra nhanh API c√≥ ho·∫°t ƒë·ªông hay kh√¥ng
    return {"message": "AmongSMS Spam Detection API", "status": "running"}


@app.get("/health")
async def health_check():
    # Ki·ªÉm tra tr·∫°ng th√°i t·∫£i m√¥ h√¨nh v√† vectorizer
    try:
        vec, model = load_artifacts()
        return {
            "status": "healthy",
            "model_loaded": True,
            "model_type": type(model).__name__
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}


# H√†m d·ª± ƒëo√°n cho m·ªôt tin nh·∫Øn ƒë∆°n l·∫ª
def predict_one(text: str):
    vec, model = load_artifacts()
    X = vec.transform([text])

    # D·ª± ƒëo√°n nh√£n
    y_pred = model.predict(X)[0]
    label = "spam" if y_pred == 1 else "ham"

    # T√≠nh x√°c su·∫•t v√† ƒë·ªô tin c·∫≠y
    prob = None
    confidence = 0.0
    
    if hasattr(model, "predict_proba"):
        prob = float(model.predict_proba(X)[0][1])
        confidence = round(prob * 100, 2) if label == "spam" else round((1 - prob) * 100, 2)
    else:
        # N·∫øu model kh√¥ng h·ªó tr·ª£ x√°c su·∫•t
        confidence = 85.0 if label == "spam" else 90.0

    # Tr√≠ch xu·∫•t top t·ª´ trong tin nh·∫Øn
    top_words = extract_top_spam_words(text, vec)
    return {
        "label": label,
        "prob": prob,
        "top_words": top_words,
        "confidence": confidence
    }


# Endpoint /predict: d·ª± ƒëo√°n m·ªôt tin nh·∫Øn
@app.post("/predict", response_model=SMSResponse)
async def predict_sms(request: SMSRequest):
    try:
        result = predict_one(request.text)
        return SMSResponse(**result)
    except Exception as e:
        raise HTTPException(500, f"L·ªói khi d·ª± ƒëo√°n: {str(e)}")


# Endpoint /stats: cung c·∫•p th√¥ng tin th·ªëng k√™ v·ªÅ c√°c t·ª´ kh√≥a spam ph·ªï bi·∫øn
@app.get("/stats")
async def get_stats():
    return {
        "spam_keywords_frequency": [
            ["free", "Xu·∫•t hi·ªán trong 89% tin nh·∫Øn spam"],
            ["win", "G·∫∑p trong 76% tin nh·∫Øn th√¥ng b√°o tr√∫ng th∆∞·ªüng"],
            ["prize", "C√≥ m·∫∑t trong 67% tin spam"],
            ["cash", "Li√™n quan ƒë·∫øn 54% tin l·ª´a ƒë·∫£o t√†i ch√≠nh"],
            ["urgent", "D√πng trong 45% tin nh·∫Øn g·∫•p g√°p"],
            ["congratulations", "Th∆∞·ªùng th·∫•y trong tin ch√∫c m·ª´ng gi·∫£"],
            ["click", "61% spam y√™u c·∫ßu nh·∫•p link"],
            ["claim", "58% spam ch·ª©a t·ª´ 'claim'"],
            ["limited", "52% spam c√≥ ∆∞u ƒë√£i gi·ªõi h·∫°n th·ªùi gian"],
            ["guaranteed", "47% spam h·ª©a h·∫πn 'ƒë·∫£m b·∫£o'"]
        ],
        "detection_tips": [
            "Tin nh·∫Øn c√≥ nhi·ªÅu t·ª´ trong danh s√°ch tr√™n th∆∞·ªùng l√† spam",
            "Spam th∆∞·ªùng th√∫c gi·ª•c h√†nh ƒë·ªông ngay l·∫≠p t·ª©c",
            "Tin nh·∫Øn h·ª£p ph√°p hi·∫øm khi d√πng t·ª´ 'FREE', 'WIN', 'PRIZE'",
            "H√£y lu√¥n ki·ªÉm tra k·ªπ c√°c th√¥ng b√°o tr√∫ng th∆∞·ªüng",
            "Kh√¥ng nh·∫•p v√†o li√™n k·∫øt t·ª´ ng∆∞·ªùi g·ª≠i l·∫°"
        ]
    }


# Endpoint x·ª≠ l√Ω file CSV (tr·∫£ k·∫øt qu·∫£ d·∫°ng JSON)
@app.post("/batch-predict-json")
async def batch_predict_json(file: UploadFile = File(...)):
    try:
        logger.info(f"ƒêang x·ª≠ l√Ω file: {file.filename}")
        
        if not file.filename.endswith('.csv'):
            raise HTTPException(400, "Ch·ªâ ch·∫•p nh·∫≠n file CSV")
        
        # ƒê·ªçc n·ªôi dung file CSV
        content = await file.read()
        csv_content = content.decode('utf-8')
        df = pd.read_csv(io.StringIO(csv_content))
        
        # X√°c ƒë·ªãnh c·ªôt ch·ª©a vƒÉn b·∫£n (text)
        text_column = 'text' if 'text' in df.columns else df.columns[0]
        logger.info(f"S·ª≠ d·ª•ng c·ªôt: {text_column}")
        
        # T·∫£i model v√† vectorizer
        vec, model = load_artifacts()
        texts = df[text_column].astype(str).fillna('').tolist()
        
        # Bi·∫øn ƒë·ªïi vƒÉn b·∫£n v√† d·ª± ƒëo√°n
        X = vec.transform(texts)
        preds = model.predict(X)
        
        results = []
        spam_count = 0
        
        # Duy·ªát t·ª´ng tin nh·∫Øn ƒë·ªÉ th·ªëng k√™
        for i, (text, pred) in enumerate(zip(texts, preds)):
            is_spam = bool(pred == 1)
            label = "spam" if is_spam else "ham"
            if is_spam:
                spam_count += 1
            
            top_words = extract_top_spam_words(text) if is_spam else []
            
            results.append({
                "id": int(i + 1),
                "text": text[:100] + "..." if len(text) > 100 else text,
                "predicted_label": label,
                "is_spam": is_spam,
                "top_spam_words": top_words,
                "confidence": 85.0 if is_spam else 90.0
            })
        
        return {
            "filename": file.filename,
            "total_messages": len(results),
            "spam_count": spam_count,
            "ham_count": len(results) - spam_count,
            "spam_rate": round((spam_count / len(results)) * 100, 2),
            "results": results,
            "success": True
        }
        
    except Exception as e:
        logger.error(f"L·ªói x·ª≠ l√Ω batch: {e}")
        raise HTTPException(500, f"L·ªói x·ª≠ l√Ω: {str(e)}")


# Endpoint x·ª≠ l√Ω file CSV (tr·∫£ k·∫øt qu·∫£ file CSV)
@app.post("/batch-predict")
async def batch_predict_endpoint(file: UploadFile = File(...)):
    try:
        logger.info(f"ƒêang t·∫°o file CSV k·∫øt qu·∫£ cho: {file.filename}")
        
        if not file.filename.endswith('.csv'):
            raise HTTPException(400, "Ch·ªâ ch·∫•p nh·∫≠n file CSV")
        
        # ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu
        content = await file.read()
        csv_content = content.decode('utf-8')
        df = pd.read_csv(io.StringIO(csv_content))
        text_column = 'text' if 'text' in df.columns else df.columns[0]
        
        vec, model = load_artifacts()
        texts = df[text_column].astype(str).fillna('').tolist()
        
        X = vec.transform(texts)
        preds = model.predict(X)
        
        # G·∫Øn k·∫øt qu·∫£ v√†o dataframe
        df["Predicted_Label"] = ["spam" if bool(p == 1) else "ham" for p in preds]
        df["Confidence"] = [85.0 if bool(p == 1) else 90.0 for p in preds]
        
        # Xu·∫•t ra file CSV m·ªõi
        output = io.StringIO()
        df.to_csv(output, index=False)
        output.seek(0)
        
        filename = f"predicted_{file.filename}"
        return StreamingResponse(
            io.BytesIO(output.getvalue().encode()),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
        
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o file CSV: {e}")
        raise HTTPException(500, f"L·ªói: {str(e)}")


# ==================== CH·∫†Y SERVER ====================
if __name__ == "__main__":
    import uvicorn
    
    print("üöÄ ƒêang kh·ªüi ƒë·ªông AmongSMS API...")
    print("üì° Server ch·∫°y t·∫°i: http://localhost:8000")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )
