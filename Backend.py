from fastapi import FastAPI, HTTPException, UploadFile, File
# FastAPI: Framework web d√πng ƒë·ªÉ t·∫°o API backend.
# HTTPException: D√πng ƒë·ªÉ tr·∫£ l·ªói HTTP (vd: 404, 500) c√≥ m√¥ t·∫£ chi ti·∫øt.
# UploadFile, File: Cho ph√©p nh·∫≠n file t·ª´ ng∆∞·ªùi d√πng khi upload CSV.
from fastapi.middleware.cors import CORSMiddleware # Cho ph√©p frontend (HTML/JS) g·ªçi API t·ª´ domain kh√°c (b·∫≠t CORS).
from pydantic import BaseModel # ƒê·ªãnh nghƒ©a c·∫•u tr√∫c d·ªØ li·ªáu v√†o/ra API
import joblib # D√πng ƒë·ªÉ load model v√† vectorizer ƒë√£ l∆∞u
import numpy as np 
import pandas as pd
import os
import io
from typing import Dict, Any, List, Optional, Union
import logging # Ghi log khi server ch·∫°y, gi√∫p debug.
from sklearn.base import BaseEstimator, TransformerMixin # T·∫°o custom vectorizer theo chu·∫©n scikit-learn.
import re

# MANUAL VECTORIZER: B·ªò BI·∫æN ƒê·ªîI (VECTORIZER) TH·ª¶ C√îNG, D√ôNG ƒê·ªÇ CHUY·ªÇN VƒÇN B·∫¢N TH√ÄNH VECTOR S·ªê.
class ManualVectorizer(BaseEstimator, TransformerMixin):
    def __init__(self, vocab=None):
        self.vocab = vocab or {} # vocab: danh s√°ch c√°c t·ª´ ƒë∆∞·ª£c d√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh
        self.vocab_index = {word: idx for idx, word in enumerate(self.vocab)} # T·∫°o ch·ªâ m·ª•c (index) cho t·ª´ng t·ª´ trong vocab
    
    def fit(self, X, y=None):# H√†m fit kh√¥ng l√†m g√¨ v√¨ vocab ƒë√£ c√≥ s·∫µn (fit ƒë√£ ƒë∆∞·ª£c l√†m trong b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω)
        return self
    
    def transform(self, X):
        if isinstance(X, str): # N·∫øu ch·ªâ truy·ªÅn v√†o 1 chu·ªói, ƒë∆∞a v√†o danh s√°ch
            X = [X]
        
        results = []
        for text in X:
            features = np.zeros(len(self.vocab))# T·∫°o vector ƒë·ªô d√†i = s·ªë t·ª´ trong vocab, kh·ªüi t·∫°o b·∫±ng 0
            words = text.lower().split()# t√°ch c√¢u th√†nh danh s√°ch t·ª´ (·ªü ƒë√¢y t√°ch ƒë∆°n gi·∫£n theo kho·∫£ng tr·∫Øng, lowercase).
            for word in words:
                if word in self.vocab_index: # N·∫øu t·ª´ n·∫±m trong vocab th√¨ tƒÉng t·∫ßn su·∫•t
                    features[self.vocab_index[word]] += 1
            results.append(features)
        
        return np.array(results)
    
    def fit_transform(self, X, y=None):# type: ignore  # kh√¥ng c·∫ßn fit n·ªØa v√¨ ƒë√£ t·ª± build Vocab 
        return self.transform(X)# h√†m bi·∫øn ƒë·ªïi d·ªØ li·ªáu ƒë·∫ßu v√†o (vƒÉn b·∫£n) th√†nh d·∫°ng s·ªë h·ªçc m√† m√¥ h√¨nh m√°y h·ªçc c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c.

    @property
    def vocabulary_(self):
        return self.vocab_index# Tr·∫£ v·ªÅ t·ª´ ƒëi·ªÉn vocab_index (ph√π h·ª£p v·ªõi chu·∫©n sklearn)

# C·∫§U H√åNH LOGGING CHO SERVER
logging.basicConfig(level=logging.INFO)# logging d√πng ƒë·ªÉ in log ho·∫°t ƒë·ªông ra console.
logger = logging.getLogger(__name__)

ARTIFACT_DIR = "artifacts" # ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh v√† vectorizer
VEC_PATH = os.path.join(ARTIFACT_DIR, "vectorizer.pkl")# ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß t·ªõi vectorizer.pkl.
MODEL_PATH = os.path.join(ARTIFACT_DIR, "spam_model.pkl")# ƒë∆∞·ªùng d·∫´n t·ªõi m√¥ h√¨nh t·ªët nh·∫•t (spam_model.pkl)

# KH·ªûI T·∫†O FASTAPI 
app = FastAPI(
    title="AmongSMS - Spam Detection API",
    description="API ph√°t hi·ªán tin nh·∫Øn r√°c (spam)",
    version="1.0.0"
)

# C·∫§U H√åNH CORS
# Cho ph√©p giao di·ªán web (frontend) ·ªü b·∫•t k·ª≥ ngu·ªìn n√†o c√≥ th·ªÉ g·ªçi API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ origins trong development
    allow_credentials=True,
    allow_methods=["*"],# Cho ph√©p t·∫•t c·∫£ c√°c lo·∫°i HTTP method
    allow_headers=["*"],# Cho ph√©p t·∫•t c·∫£ headers
)

# ƒê·ªäNH NGHƒ®A KI·ªÇU D·ªÆ LI·ªÜU  
class SMSRequest(BaseModel):
    text: str # N·ªôi dung tin nh·∫Øn ƒë·∫ßu v√†o

class SMSResponse(BaseModel):
    label: str #  K·∫øt qu·∫£ d·ª± ƒëo√°n: "spam" ho·∫∑c "ham"
    prob: Optional[float] # X√°c su·∫•t l√† spam
    top_words: List[List[Union[str, int]]] # T·ª´ kh√≥a xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
    confidence: float # M·ª©c ƒë·ªô tin c·∫≠y (%)

class BatchResponse(BaseModel): # output cho batch prediction
    filename: str
    total_messages: int
    spam_count: int
    ham_count: int
    results: List[Dict]

# CORE FUNCTIONS 
vectorizer = None
model = None

def load_artifacts():
    global vectorizer, model
    if vectorizer is None or model is None:
        if not os.path.exists(VEC_PATH) or not os.path.exists(MODEL_PATH):# N·∫øu file ko t·ªìn t·∫°i
            raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y vectorizer.pkl ho·∫∑c spam_model.pkl trong artifacts")
        vectorizer = joblib.load(VEC_PATH) # d√πng joblib load vectorizer
        model_data = joblib.load(MODEL_PATH) # d√πng joblib load model t·ªët nh·∫•t
        model = model_data['model'] if isinstance(model_data, dict) else model_data #N·∫øu model_data l√† dictionary, th√¨ ta ch·ªâ l·∫•y ph·∫ßn 'model' trong ƒë√≥. Ng∆∞·ª£c l·∫°i, n·∫øu n√≥ kh√¥ng ph·∫£i dictionary (t·ª©c l√† b·∫£n th√¢n joblib.load() tr·∫£ v·ªÅ m√¥ h√¨nh lu√¥n) ‚Üí ta g√°n tr·ª±c ti·∫øp.
        print(f"Model loaded: {type(model).__name__}") #In ra t√™n l·ªõp c·ªßa m√¥ h√¨nh v·ª´a load
        print(f"Vectorizer vocabulary size: {len(vectorizer.vocabulary_)}") # In ra k√≠ch th∆∞·ªõc vocab c·ªßa vectorizer
    return vectorizer, model

def extract_top_spam_words(text: str, top_k: int = 5):
    try:
        # Danh s√°ch t·ª´ spam ph·ªï bi·∫øn v·ªõi tr·ªçng s·ªë
        spam_keywords = {
            'free': 10, 'win': 9, 'won': 9, 'prize': 8, 'cash': 8, 'congratulations': 10,
            'claim': 7, 'urgent': 7, 'limited': 6, 'guaranteed': 6, 'click': 7, 'award': 6,
            'reward': 6, 'bonus': 5, 'discount': 5, 'offer': 5, 'deal': 4, 'sale': 4,
            'selected': 5, 'lucky': 5, 'winner': 7, 'million': 6, 'dollar': 5, 'money': 5,
            'credit': 4, 'loan': 4, 'text': 2, 'stop': 2, 'reply': 3, 'call': 3, 'now': 4,
            'today': 3, 'only': 3, 'special': 4, 'exclusive': 4, 'last': 3, 'chance': 4,
            'opportunity': 3, 'apply': 3, 'register': 3, 'sign': 3, 'subscribe': 3,
            'code': 3, 'password': 2, 'account': 2, 'premium': 6, 'subscription': 5,
            'membership': 4, 'buy': 4, 'purchase': 4, 'order': 4, 'price': 4, 'cost': 3,
            'payment': 3, 'card': 3, 'bank': 3, 'verify': 3, 'confirm': 3, 'access': 3,
            'unlock': 4, 'download': 3, 'mobile': 2, 'phone': 2, 'service': 2, 'gift': 5,
            'present': 4, 'extra': 3, 'clearance': 5, 'bargain': 4, 'promotion': 5,
            'trial': 4, 'new': 3, 'latest': 3, 'secret': 4, 'amazing': 4, 'awesome': 3,
            'best': 4, 'top': 4, 'quality': 3, 'luxury': 4, 'vip': 5, 'instant': 4,
            'quick': 3, 'easy': 3, 'profit': 5, 'income': 4, 'rich': 4, 'success': 4
        }
        
        words = text.lower().split() # T√°ch vƒÉn b·∫£n th√†nh c√°c t·ª´ ri√™ng l·∫ª v√† chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
        found_spam_words = [] # Danh s√°ch ƒë·ªÉ l∆∞u c√°c t·ª´ spam t√¨m th·∫•y
        
        # Duy·ªát qua t·ª´ng t·ª´ trong tin nh·∫Øn ƒë·ªÉ t√¨m t·ª´ spam
        for word in words:
            clean_word = re.sub(r'[^\w\s]', '', word.lower()) # L√†m s·∫°ch t·ª´: lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ ch·ªØ c√°i v√† s·ªë
            if clean_word and len(clean_word) > 2:  # Ch·ªâ x√©t t·ª´ c√≥ √≠t nh·∫•t 3 k√Ω t·ª±
                if clean_word in spam_keywords:
                    score = spam_keywords[clean_word] # L·∫•y tr·ªçng s·ªë spam c·ªßa t·ª´ n√†y
                    found_spam_words.append([clean_word, score]) # Th√™m t·ª´ v√† tr·ªçng s·ªë v√†o danh s√°ch k·∫øt qu·∫£
        
        # Lo·∫°i b·ªè c√°c t·ª´ tr√πng l·∫∑p, gi·ªØ l·∫°i b·∫£n c√≥ tr·ªçng s·ªë cao nh·∫•t
        unique_words = {}
        for word, score in found_spam_words:
            if word not in unique_words or score > unique_words[word]:# N·∫øu t·ª´ ch∆∞a c√≥ trong dict, ho·∫∑c c√≥ tr·ªçng s·ªë cao h∆°n th√¨ c·∫≠p nh·∫≠t
                unique_words[word] = score
        
        sorted_words = sorted(unique_words.items(), key=lambda x: x[1], reverse=True) # S·∫Øp x·∫øp c√°c t·ª´ theo tr·ªçng s·ªë gi·∫£m d·∫ßn
        result = [[word, score] for word, score in sorted_words[:top_k]]  # Ch·ªâ l·∫•y top_k t·ª´ quan tr·ªçng nh·∫•t
        
        # FALLBACK: n·∫øu kh√¥ng t√¨m th·∫•y t·ª´ spam, ph√¢n t√≠ch th√™m
        if not result:
            # Th·ª≠ t√¨m c√°c t·ª´ c√≥ v·∫ª spam-like
            for word in words:# Duy·ªát l·∫°i qua c√°c t·ª´ ƒë·ªÉ t√¨m t·ª´ c√≥ v·∫ª spam-like d·ª±a tr√™n pattern
                clean_word = re.sub(r'[^\w\s]', '', word.lower())
                if len(clean_word) > 3:# Ch·ªâ x√©t t·ª´ c√≥ √≠t nh·∫•t 4 k√Ω t·ª±
                    # Ki·ªÉm tra c√°c pattern spam th√¥ng th∆∞·ªùng
                    if any(pattern in clean_word for pattern in ['free', 'win', 'cash', 'prize', 'offer']):
                        result.append([clean_word, 3]) # Th√™m t·ª´ n√†y v·ªõi tr·ªçng s·ªë m·∫∑c ƒë·ªãnh th·∫•p
                        if len(result) >= top_k:# D·ª´ng khi ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng t·ª´ c·∫ßn t√¨m
                            break
        
        print(f"Spam words from '{text[:30]}...': {result}")
        return result
        
    except Exception as e:
        print(f"Error in extract_top_spam_words: {e}")
        return [['spam', 5]]  # Fallback an to√†n

def predict_one(text: str): # H√†m d·ª± ƒëo√°n cho m·ªôt tin nh·∫Øn ƒë∆°n l·∫ª
    try:
        vec, model = load_artifacts()
        X = vec.transform([text]) # Vector h√≥a

        # D·ª± ƒëo√°n nh√£n
        y_pred = model.predict(X)[0] # d√πng best model ƒë√£ train v√† h√†m predict() c·ªßa skikit-learn -> 1 or 0
        label = "spam" if y_pred == 1 else "ham"

        print(f"Prediction: {label} for text: {text[:50]}...")

        # T√≠nh x√°c su·∫•t v√† ƒë·ªô tin c·∫≠y
        confidence = 85.0
        
        if hasattr(model, "predict_proba"): #ki·ªÉm tra xem model c√≥ h√†m predict_proba kh√¥ng 
            proba = model.predict_proba(X)[0] # t√≠nh probability c·ªßa spam v√† ham
            prob = float(proba[1])
            confidence = round(prob * 100, 2) if label == "spam" else round((1 - prob) * 100, 2)
            print(f"Confidence: {confidence}%")
        else:# N·∫øu model kh√¥ng h·ªó tr·ª£ x√°c su·∫•t
            prob = 0.85 if label == "spam" else 0.15
            print(f"Using fallback confidence: {confidence}%")

        # Tr√≠ch xu·∫•t top 10 t·ª´ g√¢y spam trong tin nh·∫Øn
        top_words = []
        if label == "spam":
            top_words = extract_top_spam_words(text)
            print(f"Top spam words: {top_words}")
        
        return {
            "label": label,
            "prob": prob,
            "top_words": top_words,
            "confidence": confidence
        }
    except Exception as e:
        logger.error(f"L·ªói predict_one: {e}")
        raise e

# ENDPOINTS
# Endpoint ki·ªÉm tra nhanh API c√≥ ho·∫°t ƒë·ªông hay kh√¥ng
@app.get("/")
async def root():
    return {"message": "AmongSMS Spam Detection API", "status": "running"}

#Endpoint Ki·ªÉm tra tr·∫°ng th√°i t·∫£i m√¥ h√¨nh v√† vectorizer
@app.get("/health")
async def health_check():
    try:
        vec, model = load_artifacts()
        return {
            "status": "healthy",
            "model_loaded": True,
            "model_type": type(model).__name__
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

# Endpoint /predict: d·ª± ƒëo√°n m·ªôt tin nh·∫Øn
@app.post("/predict", response_model=SMSResponse)
async def predict_sms(request: SMSRequest):
    try:
        print(f"üîç Received prediction request: {request.text[:100]}...")
        result = predict_one(request.text)
        print(f"Prediction result: {result}")
        return SMSResponse(**result)
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        raise HTTPException(500, f"L·ªói khi d·ª± ƒëo√°n: {str(e)}")

# Endpoint x·ª≠ l√Ω file CSV (tr·∫£ k·∫øt qu·∫£ d·∫°ng JSON)
@app.post("/batch-predict-json")# Ng∆∞·ªùi d√πng s·∫Ω upload file CSV qua endpoint n√†y.
async def batch_predict_json(file: UploadFile = File(...)): # Nh·∫≠n file upload t·ª´ client
    try:
        print(f"Processing batch file: {file.filename}")
        
        content = await file.read() # ƒê·ªçc to√†n b·ªô n·ªôi dung file t·ª´ client upload
        
        # Th·ª≠ c√°c encoding kh√°c nhau ƒë·ªÉ ƒë·ªçc file CSV
        encodings = ['utf-8', 'latin-1', 'utf-16', 'windows-1252', 'cp1252']
        df = None # Kh·ªüi t·∫°o bi·∫øn l∆∞u DataFrame
        
        for encoding in encodings:
            try:
                csv_content = content.decode(encoding) # Gi·∫£i m√£ n·ªôi dung file v·ªõi encoding hi·ªán t·∫°i
                df = pd.read_csv(io.StringIO(csv_content)) # ƒê·ªçc n·ªôi dung CSV v√†o DataFrame
                print(f"Successfully decoded with {encoding}")
                break
            except Exception as e:
                print(f"Failed with {encoding}: {e}")
                continue
        
        if df is None: # Fallback: n·∫øu kh√¥ng decode ƒë∆∞·ª£c, ƒë·ªÉ pandas t·ª± detect encoding
            try:
                df = pd.read_csv(io.BytesIO(content)) # ƒê·ªçc tr·ª±c ti·∫øp t·ª´ bytes m√† kh√¥ng decode tr∆∞·ªõc
                print("Successfully read with pandas auto-detection")
            except Exception as e:
                raise Exception(f"Kh√¥ng th·ªÉ ƒë·ªçc file CSV: {str(e)}")
        
        print(f" CSV columns: {list(df.columns)}")
        print(f" Sample data: {df.head(2)}")
        
        # T√¨m c·ªôt text linh ho·∫°t - h·ªó tr·ª£ nhi·ªÅu t√™n c·ªôt kh√°c nhau
        text_column = None
        possible_columns = ['text', 'message', 'sms', 'content', 'body', 'Message', 'SMS', 'Text']
        
        for col in possible_columns: # Duy·ªát qua danh s√°ch t√™n c·ªôt c√≥ th·ªÉ c√≥
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None and len(df.columns) > 0: # Fallback: n·∫øu kh√¥ng t√¨m th·∫•y c·ªôt theo t√™n, d√πng c·ªôt ƒë·∫ßu ti√™n
            text_column = df.columns[0]
        
        # Ki·ªÉm tra xem ƒë√£ t√¨m th·∫•y c·ªôt text ch∆∞a
        if text_column is None:
            raise Exception("Kh√¥ng t√¨m th·∫•y c·ªôt ch·ª©a tin nh·∫Øn trong file CSV")
            
        print(f"Using column: {text_column}")
        
        # L·∫•y texts v√† l·ªçc b·ªè gi√° tr·ªã NaN, r·ªóng
        texts = df[text_column].astype(str).tolist() # Chuy·ªÉn c·ªôt text th√†nh list
        texts = [text.strip() for text in texts if text and text.lower() != 'nan' and text.strip()]# Lo·∫°i b·ªè gi√° tr·ªã: None, 'nan', v√† chu·ªói ch·ªâ to√†n kho·∫£ng tr·∫Øng
        
        print(f" Processing {len(texts)} messages")
        
        if len(texts) == 0: # Ki·ªÉm tra xem c√≤n tin nh·∫Øn n√†o ƒë·ªÉ x·ª≠ l√Ω kh√¥ng
            raise Exception("Kh√¥ng c√≥ tin nh·∫Øn n√†o ƒë·ªÉ x·ª≠ l√Ω")
        
        # Load model v√† predict
        vec, model = load_artifacts()
        X = vec.transform(texts)
        preds = model.predict(X)
        
        # T·∫°o k·∫øt qu·∫£
        results = []
        spam_count = 0
        
        for i, text in enumerate(texts): # Duy·ªát qua t·ª´ng tin nh·∫Øn ƒë·ªÉ x·ª≠ l√Ω chi ti·∫øt
            is_spam = bool(preds[i] == 1)  # Chuy·ªÉn prediction th√†nh boolean

            if is_spam:
                spam_count += 1
            
            # T√≠nh confidence
            confidence = 85.0
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X[i:i+1])[0]
                confidence = round(proba[1] * 100, 2) if is_spam else round(proba[0] * 100, 2)
            
            # Lu√¥n tr√≠ch xu·∫•t t·ª´ spam khi l√† spam
            top_spam_words = extract_top_spam_words(text) if is_spam else [] # Ch·ªâ extract t·ª´ spam cho tin nh·∫Øn spam ƒë·ªÉ ti·∫øt ki·ªám t√≠nh to√°n
            
            results.append({
                "id": i + 1,
                "text": text,
                "predicted_label": "spam" if is_spam else "ham",
                "is_spam": is_spam,
                "top_spam_words": top_spam_words,
                "confidence": confidence
            })
        
        # T·ªïng h·ª£p k·∫øt qu·∫£ to√†n batch
        response_data = {
            "filename": file.filename,
            "total_messages": len(results),
            "spam_count": spam_count,
            "ham_count": len(results) - spam_count,
            "spam_rate": round((spam_count / len(results)) * 100, 2) if len(results) > 0 else 0,
            "results": results,
            "success": True
        }
        
        print(f" Batch processing completed: {spam_count}/{len(results)} spam")
        return response_data
        
    except Exception as e:
        print(f" Batch error: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    import uvicorn # l√† web server ASGI (Asynchronous Server Gateway Interface).ƒê√¢y l√† server th·∫≠t s·ª± ch·∫°y ·ª©ng d·ª•ng FastAPI
    # CH·∫†Y PORT 8000
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)