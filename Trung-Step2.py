# %%
import os
import csv
import json
import joblib
import numpy as np
from typing import List, Dict, Tuple
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
random_state =42

vocab_size = 3000 

RAW_CSV = "data/raw/sms.csv"              # file gá»‘c: 2 cá»™t "label","text"
TRAIN_OUT = "data/processed/train.csv"    # output train
TEST_OUT  = "data/processed/test.csv"     # output test
VEC_PKL   = "artifacts/vectorizer.pkl"    # pickle vectorizer cho BÆ°á»›c 3/4
VOCAB_TXT = "artifacts/vocab.txt"         # vocab (tham kháº£o)
TEST_SIZE = 0.2                           # 80/20 split

# -----------------------------
# 1) STOPWORDS & CLEANING
# -----------------------------
STOPWORDS = {
    "a","an","the","is","are","am","was","were","be","been","being","i","you","he","she","it","we","they","me","him","her","us","them",
    "this","that","these","those","there","here","of","to","in","on","for","from","with","by","at","as","about","into","over","after",
    "before","between","and","or","but","if","then","so","because","while","than","though","although","not","no","do","does","did","doing",
    "done","dont","didnt","doesnt","isnt","arent","wasnt","werent","cant","cannot","my","your","his","her","its","our","their",
    "have","has","had","having","will","would","shall","should","can","could","may","might","must",
}
Contraction = {
    "im": "i am",
    "ive": "i have",
    "youre": "you are",
    "hes": "he is",
    "shes": "she is",
    "weve": "we have",
    "theyre": "they are",
    "ill": "i will",
    "youll": "you will",
    "dont": "do not",
    "cant": "cannot",
    "wont": "will not",
    "didnt": "did not",
    "couldnt": "could not",
    "shouldnt": "should not",
    "wouldnt": "would not",
    "lets": "let us"
}

def keep_letters_and_spaces(s: str) -> str:
    """
    Chá»‰ giá»¯ láº¡i chá»¯ cÃ¡i (a-z) vÃ  khoáº£ng tráº¯ng
    
    Tham sá»‘:
        s: chuá»—i Ä‘Ã£ lowercase
        
    Tráº£ vá»:
        Chuá»—i chá»‰ chá»©a a-z vÃ  space
        
    VÃ­ dá»¥:
        "hello123!@#world" -> "hello   world"
    """
    out = []
    for ch in s:
        if 'a' <= ch <= 'z' or ch == ' ':
            out.append(ch)
        else:
            out.append(' ')
    return ''.join(out)


def clean_text(text: str, stopwords: set) -> str: #LÃ m sáº¡ch vÄƒn báº£n: lowercase -> loáº¡i kÃ½ tá»± Ä‘áº·c biá»‡t -> loáº¡i stopword
    """
        text: vÄƒn báº£n cáº§n lÃ m sáº¡ch
        stopwords: táº­p há»£p cÃ¡c tá»« dá»«ng
        
    Tráº£ vá»:
        VÄƒn báº£n Ä‘Ã£ lÃ m sáº¡ch
        
    CÃ¡c bÆ°á»›c:
        1. Chuyá»ƒn vá» chá»¯ thÆ°á»ng
        2. Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, chá»‰ giá»¯ a-z vÃ  space
        3. Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        4. Loáº¡i bá» stopwords
        5. Loáº¡i bá» tá»« cÃ³ Ä‘á»™ dÃ i <= 1
    """
    text = text.lower()# BÆ°á»›c 1: lowercase
    

    text = keep_letters_and_spaces(text)# BÆ°á»›c 2: chá»‰ giá»¯ chá»¯ cÃ¡i vÃ  space
    
    text = ' '.join(text.split()) # BÆ°á»›c 3: loáº¡i khoáº£ng tráº¯ng thá»«a
    
    
    words = text.split()
    words = [w for w in words if w not in stopwords and len(w) > 1]# BÆ°á»›c 4 & 5: tÃ¡ch tá»«, loáº¡i stopwords vÃ  tá»« ngáº¯n
    
    return ' '.join(words)


# -----------------------------
# *) Äá»ŒC VÃ€ LÃ€M Sáº CH Dá»® LIá»†U
# -----------------------------
def load_and_clean_data(csv_path: str, stopwords: set) -> Tuple[List[str], List[int]]:
    """
    Äá»c file CSV vÃ  lÃ m sáº¡ch dá»¯ liá»‡u
    
    Tham sá»‘:
        csv_path: Ä‘Æ°á»ng dáº«n file CSV (cá»™t 1: label, cá»™t 2: text)
        stopwords: táº­p stopwords
        
    Tráº£ vá»:
        (texts, labels) - danh sÃ¡ch vÄƒn báº£n Ä‘Ã£ lÃ m sáº¡ch vÃ  nhÃ£n (0=ham, 1=spam)
    """
    print(f"\nğŸ“– Äá»c dá»¯ liá»‡u tá»«: {csv_path}")
    
    texts = []
    labels = []
    
    # Äá»c file CSV
    with open(csv_path, 'r', encoding='latin-1') as f:
        reader = csv.reader(f)
        header = next(reader)  # bá» qua header
        
        for row in reader:
            if len(row) < 2:
                continue
                
            label_str = row[0].strip().lower()  # 'ham' hoáº·c 'spam'
            raw_text = row[1].strip()
            
            
            cleaned = clean_text(raw_text, stopwords) # LÃ m sáº¡ch vÄƒn báº£n
            
            
            label_int = 1 if label_str == 'spam' else 0 # Chuyá»ƒn label thÃ nh sá»‘: ham=0, spam=1
            
                             
            if cleaned:# Chá»‰ giá»¯ láº¡i náº¿u vÄƒn báº£n khÃ´ng rá»—ng sau khi lÃ m sáº¡ch
                texts.append(cleaned)
                labels.append(label_int)
    
    print(f"âœ… Äá»c thÃ nh cÃ´ng {len(texts)} tin nháº¯n")
    print(f"   - HAM (0): {labels.count(0)} tin")
    print(f"   - SPAM (1): {labels.count(1)} tin")
    
    return texts, labels


# -----------------------------
# 3) CHIA TRAIN/TEST
# -----------------------------
def split_train_test(texts: List[str], labels: List[int], 
                     test_size: float = 0.2, 
                     random_state: int = 42) -> Tuple:
    """
    Chia dá»¯ liá»‡u thÃ nh táº­p train vÃ  test
    
    Tham sá»‘:
        texts: danh sÃ¡ch vÄƒn báº£n
        labels: danh sÃ¡ch nhÃ£n
        test_size: tá»· lá»‡ test (0.2 = 20%)
        random_state: seed cho random
        
    Tráº£ vá»:
        X_train, X_test, y_train, y_test
    """
    print(f"\n Chia dá»¯ liá»‡u: {int((1-test_size)*100)}% train, {int(test_size*100)}% test")
    
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, 
        test_size=test_size, 
        random_state=random_state,
        stratify=labels  # Ä‘áº£m báº£o tá»· lá»‡ ham/spam Ä‘á»u trong train vÃ  test
    )
    
    print(f" Train: {len(X_train)} máº«u")
    print(f" Test:  {len(X_test)} máº«u")
    
    return X_train, X_test, y_train, y_test


# -----------------------------
# 4) VECTOR HÃ“A VÄ‚N Báº¢N
# -----------------------------
def create_vectorizer(vocab_size: int = 3000, method: str = 'tfidf'):
    """
    Táº¡o vectorizer Ä‘á»ƒ chuyá»ƒn vÄƒn báº£n thÃ nh vector sá»‘
    
    Tham sá»‘:
        vocab_size: sá»‘ lÆ°á»£ng tá»« trong vocabulary
        method: 'tfidf' hoáº·c 'count'
        
    Tráº£ vá»:
        Vectorizer object
        
    Giáº£i thÃ­ch:
        - TF-IDF: Term Frequency - Inverse Document Frequency
          + Äo lÆ°á»ng má»©c Ä‘á»™ quan trá»ng cá»§a tá»« trong vÄƒn báº£n
          + Tá»« xuáº¥t hiá»‡n nhiá»u trong 1 vÄƒn báº£n nhÆ°ng Ã­t trong toÃ n bá»™ â†’ quan trá»ng
          
        - Count: ÄÆ¡n giáº£n Ä‘áº¿m sá»‘ láº§n xuáº¥t hiá»‡n cá»§a tá»«
    """
    print(f"\n Táº¡o vectorizer ({method.upper()}) vá»›i vocab_size={vocab_size}")
    
    if method == 'tfidf':
        vectorizer = TfidfVectorizer(
            max_features=vocab_size,  # giá»›i háº¡n sá»‘ tá»«
            ngram_range=(1, 2),       # unigram vÃ  bigram (tá»« Ä‘Æ¡n vÃ  cá»¥m 2 tá»«)
            min_df=2,                 # tá»« pháº£i xuáº¥t hiá»‡n Ã­t nháº¥t 2 láº§n
            max_df=0.95               # loáº¡i tá»« xuáº¥t hiá»‡n quÃ¡ nhiá»u (>95% vÄƒn báº£n)
        )
    else:
        vectorizer = CountVectorizer(
            max_features=vocab_size,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.95
        )
    
    return vectorizer





