{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "random_state =42\n",
    "\n",
    "vocab_size = 3000 \n",
    "\n",
    "RAW_CSV = \"data/raw/sms.csv\"              # file g·ªëc: 2 c·ªôt \"label\",\"text\"\n",
    "TRAIN_OUT = \"data/processed/train.csv\"    # output train\n",
    "TEST_OUT  = \"data/processed/test.csv\"     # output test\n",
    "VEC_PKL   = \"artifacts/vectorizer.pkl\"    # pickle vectorizer cho B∆∞·ªõc 3/4\n",
    "VOCAB_TXT = \"artifacts/vocab.txt\"         # vocab (tham kh·∫£o)\n",
    "TEST_SIZE = 0.2                           # 80/20 split\n",
    "\n",
    "# -----------------------------\n",
    "# 1) STOPWORDS & CLEANING\n",
    "# -----------------------------\n",
    "STOPWORDS = {\n",
    "    \"a\",\"an\",\"the\",\"is\",\"are\",\"am\",\"was\",\"were\",\"be\",\"been\",\"being\",\"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\n",
    "    \"this\",\"that\",\"these\",\"those\",\"there\",\"here\",\"of\",\"to\",\"in\",\"on\",\"for\",\"from\",\"with\",\"by\",\"at\",\"as\",\"about\",\"into\",\"over\",\"after\",\n",
    "    \"before\",\"between\",\"and\",\"or\",\"but\",\"if\",\"then\",\"so\",\"because\",\"while\",\"than\",\"though\",\"although\",\"not\",\"no\",\"do\",\"does\",\"did\",\"doing\",\n",
    "    \"done\",\"dont\",\"didnt\",\"doesnt\",\"isnt\",\"arent\",\"wasnt\",\"werent\",\"cant\",\"cannot\",\"my\",\"your\",\"his\",\"her\",\"its\",\"our\",\"their\",\n",
    "    \"have\",\"has\",\"had\",\"having\",\"will\",\"would\",\"shall\",\"should\",\"can\",\"could\",\"may\",\"might\",\"must\",\n",
    "    \"im\",\"ive\",\"youre\",\"hes\",\"shes\",\"weve\",\"theyre\",\"ill\",\"youll\",\"dont\",\"cant\",\"wont\",\"didnt\",\"couldnt\",\"shouldnt\",\"wouldnt\",\"lets\"\n",
    "}\n",
    "\n",
    "def keep_letters_and_spaces(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ch·ªâ gi·ªØ l·∫°i ch·ªØ c√°i (a-z) v√† kho·∫£ng tr·∫Øng\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        s: chu·ªói ƒë√£ lowercase\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        Chu·ªói ch·ªâ ch·ª©a a-z v√† space\n",
    "        \n",
    "    V√≠ d·ª•:\n",
    "        \"hello123!@#world\" -> \"hello   world\"\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if 'a' <= ch <= 'z' or ch == ' ':\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append(' ')\n",
    "    return ''.join(out)\n",
    "\n",
    "\n",
    "def clean_text(text: str, stopwords: set) -> str: #L√†m s·∫°ch vƒÉn b·∫£n: lowercase -> lo·∫°i k√Ω t·ª± ƒë·∫∑c bi·ªát -> lo·∫°i stopword\n",
    "    \"\"\"\n",
    "        text: vƒÉn b·∫£n c·∫ßn l√†m s·∫°ch\n",
    "        stopwords: t·∫≠p h·ª£p c√°c t·ª´ d·ª´ng\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        VƒÉn b·∫£n ƒë√£ l√†m s·∫°ch\n",
    "        \n",
    "    C√°c b∆∞·ªõc:\n",
    "        1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "        2. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ a-z v√† space\n",
    "        3. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "        4. Lo·∫°i b·ªè stopwords\n",
    "        5. Lo·∫°i b·ªè t·ª´ c√≥ ƒë·ªô d√†i <= 1\n",
    "    \"\"\"\n",
    "    text = text.lower()# B∆∞·ªõc 1: lowercase\n",
    "    \n",
    "\n",
    "    text = keep_letters_and_spaces(text)# B∆∞·ªõc 2: ch·ªâ gi·ªØ ch·ªØ c√°i v√† space\n",
    "    \n",
    "    text = ' '.join(text.split()) # B∆∞·ªõc 3: lo·∫°i kho·∫£ng tr·∫Øng th·ª´a\n",
    "    \n",
    "    \n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 1]# B∆∞·ªõc 4 & 5: t√°ch t·ª´, lo·∫°i stopwords v√† t·ª´ ng·∫Øn\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# *) ƒê·ªåC V√Ä L√ÄM S·∫†CH D·ªÆ LI·ªÜU\n",
    "# -----------------------------\n",
    "def load_and_clean_data(csv_path: str, stopwords: set) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    ƒê·ªçc file CSV v√† l√†m s·∫°ch d·ªØ li·ªáu\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        csv_path: ƒë∆∞·ªùng d·∫´n file CSV (c·ªôt 1: label, c·ªôt 2: text)\n",
    "        stopwords: t·∫≠p stopwords\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        (texts, labels) - danh s√°ch vƒÉn b·∫£n ƒë√£ l√†m s·∫°ch v√† nh√£n (0=ham, 1=spam)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìñ ƒê·ªçc d·ªØ li·ªáu t·ª´: {csv_path}\")\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # ƒê·ªçc file CSV\n",
    "    with open(csv_path, 'r', encoding='latin-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)  # b·ªè qua header\n",
    "        \n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "                \n",
    "            label_str = row[0].strip().lower()  # 'ham' ho·∫∑c 'spam'\n",
    "            raw_text = row[1].strip()\n",
    "            \n",
    "            \n",
    "            cleaned = clean_text(raw_text, stopwords) # L√†m s·∫°ch vƒÉn b·∫£n\n",
    "            \n",
    "            \n",
    "            label_int = 1 if label_str == 'spam' else 0 # Chuy·ªÉn label th√†nh s·ªë: ham=0, spam=1\n",
    "            \n",
    "                             \n",
    "            if cleaned:# Ch·ªâ gi·ªØ l·∫°i n·∫øu vƒÉn b·∫£n kh√¥ng r·ªóng sau khi l√†m s·∫°ch\n",
    "                texts.append(cleaned)\n",
    "                labels.append(label_int)\n",
    "    \n",
    "    print(f\"‚úÖ ƒê·ªçc th√†nh c√¥ng {len(texts)} tin nh·∫Øn\")\n",
    "    print(f\"   - HAM (0): {labels.count(0)} tin\")\n",
    "    print(f\"   - SPAM (1): {labels.count(1)} tin\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) CHIA TRAIN/TEST\n",
    "# -----------------------------\n",
    "def split_train_test(texts: List[str], labels: List[int], \n",
    "                     test_size: float = 0.2, \n",
    "                     random_state: int = 42) -> Tuple:\n",
    "    \"\"\"\n",
    "    Chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† test\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        texts: danh s√°ch vƒÉn b·∫£n\n",
    "        labels: danh s√°ch nh√£n\n",
    "        test_size: t·ª∑ l·ªá test (0.2 = 20%)\n",
    "        random_state: seed cho random\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    print(f\"\\n Chia d·ªØ li·ªáu: {int((1-test_size)*100)}% train, {int(test_size*100)}% test\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=labels  # ƒë·∫£m b·∫£o t·ª∑ l·ªá ham/spam ƒë·ªÅu trong train v√† test\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Train: {len(X_train)} m·∫´u\")\n",
    "    print(f\"‚úÖ Test:  {len(X_test)} m·∫´u\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) VECTOR H√ìA VƒÇN B·∫¢N\n",
    "# -----------------------------\n",
    "def create_vectorizer(vocab_size: int = 3000, method: str = 'tfidf'):\n",
    "    \"\"\"\n",
    "    T·∫°o vectorizer ƒë·ªÉ chuy·ªÉn vƒÉn b·∫£n th√†nh vector s·ªë\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        vocab_size: s·ªë l∆∞·ª£ng t·ª´ trong vocabulary\n",
    "        method: 'tfidf' ho·∫∑c 'count'\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        Vectorizer object\n",
    "        \n",
    "    Gi·∫£i th√≠ch:\n",
    "        - TF-IDF: Term Frequency - Inverse Document Frequency\n",
    "          + ƒêo l∆∞·ªùng m·ª©c ƒë·ªô quan tr·ªçng c·ªßa t·ª´ trong vƒÉn b·∫£n\n",
    "          + T·ª´ xu·∫•t hi·ªán nhi·ªÅu trong 1 vƒÉn b·∫£n nh∆∞ng √≠t trong to√†n b·ªô ‚Üí quan tr·ªçng\n",
    "          \n",
    "        - Count: ƒê∆°n gi·∫£n ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¢ T·∫°o vectorizer ({method.upper()}) v·ªõi vocab_size={vocab_size}\")\n",
    "    \n",
    "    if method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=vocab_size,  # gi·ªõi h·∫°n s·ªë t·ª´\n",
    "            ngram_range=(1, 2),       # unigram v√† bigram (t·ª´ ƒë∆°n v√† c·ª•m 2 t·ª´)\n",
    "            min_df=2,                 # t·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t 2 l·∫ßn\n",
    "            max_df=0.95               # lo·∫°i t·ª´ xu·∫•t hi·ªán qu√° nhi·ªÅu (>95% vƒÉn b·∫£n)\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_features=vocab_size,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def fit_and_transform(vectorizer, X_train: List[str], X_test: List[str]):\n",
    "    \"\"\"\n",
    "    Fit vectorizer tr√™n train v√† transform c·∫£ train l·∫´n test\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        vectorizer: TfidfVectorizer ho·∫∑c CountVectorizer\n",
    "        X_train: danh s√°ch vƒÉn b·∫£n train\n",
    "        X_test: danh s√°ch vƒÉn b·∫£n test\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        X_train_vec, X_test_vec (d·∫°ng sparse matrix)\n",
    "        \n",
    "    L∆∞u √Ω:\n",
    "        - Fit ch·ªâ tr√™n train ƒë·ªÉ tr√°nh data leakage\n",
    "        - Transform c·∫£ train v√† test b·∫±ng c√πng vocabulary\n",
    "    \"\"\"\n",
    "    print(\"‚öôÔ∏è ƒêang fit vectorizer tr√™n t·∫≠p train...\")\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    print(\"‚öôÔ∏è ƒêang transform t·∫≠p test...\")\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"‚úÖ Train vector shape: {X_train_vec.shape}\")\n",
    "    print(f\"‚úÖ Test vector shape:  {X_test_vec.shape}\")\n",
    "    print(f\"üìö Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "    \n",
    "    return X_train_vec, X_test_vec\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) L∆ØU D·ªÆ LI·ªÜU\n",
    "# -----------------------------\n",
    "def save_processed_data(X_train, y_train, X_test, y_test, \n",
    "                       train_out: str, test_out: str):\n",
    "    \"\"\"\n",
    "    L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ra file CSV\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        X_train, y_train: d·ªØ li·ªáu train (vƒÉn b·∫£n v√† nh√£n)\n",
    "        X_test, y_test: d·ªØ li·ªáu test\n",
    "        train_out: ƒë∆∞·ªùng d·∫´n file train output\n",
    "        test_out: ƒë∆∞·ªùng d·∫´n file test output\n",
    "    \"\"\"\n",
    "    print(f\"\\n L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω...\")\n",
    "    \n",
    "    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "    os.makedirs(os.path.dirname(train_out), exist_ok=True)\n",
    "    \n",
    "    # L∆∞u train\n",
    "    train_df = pd.DataFrame({\n",
    "        'text': X_train,\n",
    "        'label': y_train\n",
    "    })\n",
    "    train_df.to_csv(train_out, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u train: {train_out}\")\n",
    "    \n",
    "    # L∆∞u test\n",
    "    test_df = pd.DataFrame({\n",
    "        'text': X_test,\n",
    "        'label': y_test\n",
    "    })\n",
    "    test_df.to_csv(test_out, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u test: {test_out}\")\n",
    "\n",
    "\n",
    "def save_vectorizer_and_vocab(vectorizer, vec_pkl: str, vocab_txt: str):\n",
    "    \"\"\"\n",
    "    L∆∞u vectorizer v√† vocabulary\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        vectorizer: ƒë·ªëi t∆∞·ª£ng vectorizer ƒë√£ fit\n",
    "        vec_pkl: ƒë∆∞·ªùng d·∫´n l∆∞u vectorizer (pickle)\n",
    "        vocab_txt: ƒë∆∞·ªùng d·∫´n l∆∞u vocabulary (text)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíæ L∆∞u vectorizer v√† vocabulary...\")\n",
    "    \n",
    "    # T·∫°o th∆∞ m·ª•c\n",
    "    os.makedirs(os.path.dirname(vec_pkl), exist_ok=True)\n",
    "    \n",
    "    # L∆∞u vectorizer\n",
    "    joblib.dump(vectorizer, vec_pkl)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u vectorizer: {vec_pkl}\")\n",
    "    \n",
    "    # L∆∞u vocabulary\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    with open(vocab_txt, 'w', encoding='utf-8') as f:\n",
    "        for word in vocab:\n",
    "            f.write(word + '\\n')\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u vocabulary ({len(vocab)} t·ª´): {vocab_txt}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) PIPELINE CH√çNH\n",
    "# -----------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    H√†m ch√≠nh: ch·∫°y to√†n b·ªô pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PIPELINE TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU SMS SPAM\".center(70))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Set random seed\n",
    "    random.seed(RANDOM_STATE)\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "    # B∆∞·ªõc 1: ƒê·ªçc v√† l√†m s·∫°ch d·ªØ li·ªáu\n",
    "    texts, labels = load_and_clean_data(RAW_CSV, STOPWORDS)\n",
    "    \n",
    "    # B∆∞·ªõc 2: Chia train/test\n",
    "    X_train, X_test, y_train, y_test = split_train_test(\n",
    "        texts, labels, \n",
    "        test_size=TEST_SIZE, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    \n",
    "    save_processed_data(X_train, y_train, X_test, y_test, TRAIN_OUT, TEST_OUT) # B∆∞·ªõc 3: L∆∞u d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch\n",
    "    \n",
    "   \n",
    "    vectorizer = create_vectorizer(vocab_size=VOCAB_SIZE, method='tfidf') # B∆∞·ªõc 4: T·∫°o v√† fit vectorizer\n",
    "    X_train_vec, X_test_vec = fit_and_transform(vectorizer, X_train, X_test)\n",
    "    \n",
    "   \n",
    "    save_vectorizer_and_vocab(vectorizer, VEC_PKL, VOCAB_TXT) # B∆∞·ªõc 5: L∆∞u vectorizer v√† vocabulary\n",
    "    \n",
    "    # Th·ªëng k√™ cu·ªëi c√πng\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" T·ªîNG K·∫æT\".center(70))\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úÖ T·ªïng s·ªë m·∫´u: {len(texts)}\")\n",
    "    print(f\"‚úÖ Train set: {len(X_train)} m·∫´u\")\n",
    "    print(f\"‚úÖ Test set: {len(X_test)} m·∫´u\")\n",
    "    print(f\"‚úÖ Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "    print(f\"‚úÖ Vector shape: ({len(X_train)}, {len(vectorizer.vocabulary_)})\")\n",
    "    print(\"\\nüìÅ Files ƒë√£ t·∫°o:\")\n",
    "    print(f\"   - {TRAIN_OUT}\")\n",
    "    print(f\"   - {TEST_OUT}\")\n",
    "    print(f\"   - {VEC_PKL}\")\n",
    "    print(f\"   - {VOCAB_TXT}\")\n",
    "    print(\"\\n‚ú® Pipeline ho√†n th√†nh!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return X_train_vec, X_test_vec, y_train, y_test, vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079fb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2c35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a88e8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/Admin/AppData/Local/Programs/Python/Python313/python3.13t.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "visited[start] = False\n",
    "while (visited[end] == False):\n",
    "    for x in range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a61101",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "item1 = tuple([1,2]); item2 = tuple([13,2])\n",
    "ds.append(item1)\n",
    "ds.append(item2)\n",
    "visited =[]\n",
    "for _ in range(n):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e3034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
