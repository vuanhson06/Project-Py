# -----------------------------
# 1) Tokenize function
# -----------------------------
def tokenize(text: str) -> List[str]:
    """
    Nhận vào một chuỗi văn bản, sau đó:
      - Chuyển toàn bộ về chữ thường
      - Giữ lại ký tự chữ cái và khoảng trắng
      - Tách thành các từ (split)
      - Loại bỏ từ rỗng và stopwords
    Trả về: danh sách các từ (tokens)
    """
    # Đưa tất cả về chữ thường
    text = text.lower()

    # Làm sạch: chỉ giữ lại ký tự chữ và khoảng trắng
    text = keep_letters_and_spaces(text)

    # Tách từ theo dấu cách
    words = text.split()

    # Bỏ stopwords và từ rỗng
    return [w for w in words if w and (w not in STOPWORDS)]


# -----------------------------
# 2) Đọc dữ liệu gốc (raw data)
# -----------------------------
def read_raw_csv(path: str) -> Tuple[List[str], List[str]]:
    """
    Đọc file CSV có 2 cột: 'label' và 'text'
    Trả về 2 danh sách song song:
      - labels: chứa nhãn (spam / ham)
      - texts: chứa nội dung tin nhắn
    """
    labels, texts = [], []

    with open(path, 'r', encoding='utf-8', newline='') as f:
        reader = csv.DictReader(f)

        # Kiểm tra xem file có đủ cột cần thiết không
        assert "label" in reader.fieldnames and "text" in reader.fieldnames, \
            "CSV phải có cột 'label' và 'text'"

        # Duyệt từng dòng để lấy dữ liệu
        for row in reader:
            labels.append(row["label"].strip())
            texts.append(row["text"].strip())

    return labels, texts


