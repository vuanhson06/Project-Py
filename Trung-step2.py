{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "random_state =42\n",
    "\n",
    "vocab_size = 3000 \n",
    "\n",
    "RAW_CSV = \"data/raw/sms.csv\"              # file g·ªëc: 2 c·ªôt \"label\",\"text\"\n",
    "TRAIN_OUT = \"data/processed/train.csv\"    # output train\n",
    "TEST_OUT  = \"data/processed/test.csv\"     # output test\n",
    "VEC_PKL   = \"artifacts/vectorizer.pkl\"    # pickle vectorizer cho B∆∞·ªõc 3/4\n",
    "VOCAB_TXT = \"artifacts/vocab.txt\"         # vocab (tham kh·∫£o)\n",
    "TEST_SIZE = 0.2                           # 80/20 split\n",
    "\n",
    "# -----------------------------\n",
    "# 1) STOPWORDS & CLEANING\n",
    "# -----------------------------\n",
    "STOPWORDS = {\n",
    "    \"a\",\"an\",\"the\",\"is\",\"are\",\"am\",\"was\",\"were\",\"be\",\"been\",\"being\",\"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\n",
    "    \"this\",\"that\",\"these\",\"those\",\"there\",\"here\",\"of\",\"to\",\"in\",\"on\",\"for\",\"from\",\"with\",\"by\",\"at\",\"as\",\"about\",\"into\",\"over\",\"after\",\n",
    "    \"before\",\"between\",\"and\",\"or\",\"but\",\"if\",\"then\",\"so\",\"because\",\"while\",\"than\",\"though\",\"although\",\"not\",\"no\",\"do\",\"does\",\"did\",\"doing\",\n",
    "    \"done\",\"dont\",\"didnt\",\"doesnt\",\"isnt\",\"arent\",\"wasnt\",\"werent\",\"cant\",\"cannot\",\"my\",\"your\",\"his\",\"her\",\"its\",\"our\",\"their\",\n",
    "    \"have\",\"has\",\"had\",\"having\",\"will\",\"would\",\"shall\",\"should\",\"can\",\"could\",\"may\",\"might\",\"must\",\n",
    "    \"im\",\"ive\",\"youre\",\"hes\",\"shes\",\"weve\",\"theyre\",\"ill\",\"youll\",\"dont\",\"cant\",\"wont\",\"didnt\",\"couldnt\",\"shouldnt\",\"wouldnt\",\"lets\"\n",
    "}\n",
    "\n",
    "def keep_letters_and_spaces(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ch·ªâ gi·ªØ l·∫°i ch·ªØ c√°i (a-z) v√† kho·∫£ng tr·∫Øng\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        s: chu·ªói ƒë√£ lowercase\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        Chu·ªói ch·ªâ ch·ª©a a-z v√† space\n",
    "        \n",
    "    V√≠ d·ª•:\n",
    "        \"hello123!@#world\" -> \"hello   world\"\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if 'a' <= ch <= 'z' or ch == ' ':\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append(' ')\n",
    "    return ''.join(out)\n",
    "\n",
    "\n",
    "def clean_text(text: str, stopwords: set) -> str: #L√†m s·∫°ch vƒÉn b·∫£n: lowercase -> lo·∫°i k√Ω t·ª± ƒë·∫∑c bi·ªát -> lo·∫°i stopword\n",
    "    \"\"\"\n",
    "        text: vƒÉn b·∫£n c·∫ßn l√†m s·∫°ch\n",
    "        stopwords: t·∫≠p h·ª£p c√°c t·ª´ d·ª´ng\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        VƒÉn b·∫£n ƒë√£ l√†m s·∫°ch\n",
    "        \n",
    "    C√°c b∆∞·ªõc:\n",
    "        1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "        2. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ a-z v√† space\n",
    "        3. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "        4. Lo·∫°i b·ªè stopwords\n",
    "        5. Lo·∫°i b·ªè t·ª´ c√≥ ƒë·ªô d√†i <= 1\n",
    "    \"\"\"\n",
    "    text = text.lower()# B∆∞·ªõc 1: lowercase\n",
    "    \n",
    "\n",
    "    text = keep_letters_and_spaces(text)# B∆∞·ªõc 2: ch·ªâ gi·ªØ ch·ªØ c√°i v√† space\n",
    "    \n",
    "    text = ' '.join(text.split()) # B∆∞·ªõc 3: lo·∫°i kho·∫£ng tr·∫Øng th·ª´a\n",
    "    \n",
    "    \n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 1]# B∆∞·ªõc 4 & 5: t√°ch t·ª´, lo·∫°i stopwords v√† t·ª´ ng·∫Øn\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# *) ƒê·ªåC V√Ä L√ÄM S·∫†CH D·ªÆ LI·ªÜU\n",
    "# -----------------------------\n",
    "def load_and_clean_data(csv_path: str, stopwords: set) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    ƒê·ªçc file CSV v√† l√†m s·∫°ch d·ªØ li·ªáu\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        csv_path: ƒë∆∞·ªùng d·∫´n file CSV (c·ªôt 1: label, c·ªôt 2: text)\n",
    "        stopwords: t·∫≠p stopwords\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        (texts, labels) - danh s√°ch vƒÉn b·∫£n ƒë√£ l√†m s·∫°ch v√† nh√£n (0=ham, 1=spam)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìñ ƒê·ªçc d·ªØ li·ªáu t·ª´: {csv_path}\")\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # ƒê·ªçc file CSV\n",
    "    with open(csv_path, 'r', encoding='latin-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)  # b·ªè qua header\n",
    "        \n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "                \n",
    "            label_str = row[0].strip().lower()  # 'ham' ho·∫∑c 'spam'\n",
    "            raw_text = row[1].strip()\n",
    "            \n",
    "            \n",
    "            cleaned = clean_text(raw_text, stopwords) # L√†m s·∫°ch vƒÉn b·∫£n\n",
    "            \n",
    "            \n",
    "            label_int = 1 if label_str == 'spam' else 0 # Chuy·ªÉn label th√†nh s·ªë: ham=0, spam=1\n",
    "            \n",
    "                             \n",
    "            if cleaned:# Ch·ªâ gi·ªØ l·∫°i n·∫øu vƒÉn b·∫£n kh√¥ng r·ªóng sau khi l√†m s·∫°ch\n",
    "                texts.append(cleaned)\n",
    "                labels.append(label_int)\n",
    "    \n",
    "    print(f\"‚úÖ ƒê·ªçc th√†nh c√¥ng {len(texts)} tin nh·∫Øn\")\n",
    "    print(f\"   - HAM (0): {labels.count(0)} tin\")\n",
    "    print(f\"   - SPAM (1): {labels.count(1)} tin\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) CHIA TRAIN/TEST\n",
    "# -----------------------------\n",
    "def split_train_test(texts: List[str], labels: List[int], \n",
    "                     test_size: float = 0.2, \n",
    "                     random_state: int = 42) -> Tuple:\n",
    "    \"\"\"\n",
    "    Chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† test\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        texts: danh s√°ch vƒÉn b·∫£n\n",
    "        labels: danh s√°ch nh√£n\n",
    "        test_size: t·ª∑ l·ªá test (0.2 = 20%)\n",
    "        random_state: seed cho random\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    print(f\"\\n Chia d·ªØ li·ªáu: {int((1-test_size)*100)}% train, {int(test_size*100)}% test\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=labels  # ƒë·∫£m b·∫£o t·ª∑ l·ªá ham/spam ƒë·ªÅu trong train v√† test\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Train: {len(X_train)} m·∫´u\")\n",
    "    print(f\"‚úÖ Test:  {len(X_test)} m·∫´u\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) VECTOR H√ìA VƒÇN B·∫¢N\n",
    "# -----------------------------\n",
    "def create_vectorizer(vocab_size: int = 3000, method: str = 'tfidf'):\n",
    "    \"\"\"\n",
    "    T·∫°o vectorizer ƒë·ªÉ chuy·ªÉn vƒÉn b·∫£n th√†nh vector s·ªë\n",
    "    \n",
    "    Tham s·ªë:\n",
    "        vocab_size: s·ªë l∆∞·ª£ng t·ª´ trong vocabulary\n",
    "        method: 'tfidf' ho·∫∑c 'count'\n",
    "        \n",
    "    Tr·∫£ v·ªÅ:\n",
    "        Vectorizer object\n",
    "        \n",
    "    Gi·∫£i th√≠ch:\n",
    "        - TF-IDF: Term Frequency - Inverse Document Frequency\n",
    "          + ƒêo l∆∞·ªùng m·ª©c ƒë·ªô quan tr·ªçng c·ªßa t·ª´ trong vƒÉn b·∫£n\n",
    "          + T·ª´ xu·∫•t hi·ªán nhi·ªÅu trong 1 vƒÉn b·∫£n nh∆∞ng √≠t trong to√†n b·ªô ‚Üí quan tr·ªçng\n",
    "          \n",
    "        - Count: ƒê∆°n gi·∫£n ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¢ T·∫°o vectorizer ({method.upper()}) v·ªõi vocab_size={vocab_size}\")\n",
    "    \n",
    "    if method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=vocab_size,  # gi·ªõi h·∫°n s·ªë t·ª´\n",
    "            ngram_range=(1, 2),       # unigram v√† bigram (t·ª´ ƒë∆°n v√† c·ª•m 2 t·ª´)\n",
    "            min_df=2,                 # t·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t 2 l·∫ßn\n",
    "            max_df=0.95               # lo·∫°i t·ª´ xu·∫•t hi·ªán qu√° nhi·ªÅu (>95% vƒÉn b·∫£n)\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_features=vocab_size,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
